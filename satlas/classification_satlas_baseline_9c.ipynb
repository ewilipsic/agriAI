{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502c0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca2fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, mask_type='random', augment=False, target_size=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mask_type = mask_type\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "       \n",
    "        \n",
    "        # Band names in order (12 bands total)\n",
    "        self.bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
    "        \n",
    "        self.band_resolutions = {\n",
    "            'B1': 60, 'B2': 10, 'B3': 10, 'B4': 10,\n",
    "            'B5': 20, 'B6': 20, 'B7': 20, 'B8': 10,\n",
    "            'B8A': 20, 'B9': 60, 'B11': 20, 'B12': 20\n",
    "        }\n",
    "        \n",
    "        self.samples = []\n",
    "        for region_folder in tqdm(glob.glob(os.path.join(root_dir, '*'))):\n",
    "            if os.path.isdir(region_folder):\n",
    "                if(region_folder[-3:] == \"RPH\"): label = \"RPH\"\n",
    "                elif(region_folder[-5:] == \"Blast\"): label = \"Blast\"\n",
    "                elif(region_folder[-4:] == \"Rust\"): label = \"Rust\"\n",
    "                elif(region_folder[-5:] == \"Aphid\"): label = \"Aphid\"\n",
    "                else: continue\n",
    "\n",
    "                for timestamp_folder in glob.glob(os.path.join(region_folder, '*')):\n",
    "                    if os.path.isdir(timestamp_folder):\n",
    "                        # Check if all bands exist\n",
    "                        band_paths = {band: os.path.join(timestamp_folder, f'{band}.tif') \n",
    "                                     for band in self.bands}\n",
    "                        if all(os.path.exists(p) for p in band_paths.values()):\n",
    "                            self.samples.append((band_paths,label))\n",
    "        \n",
    "        print(f\"Found {len(self.samples)} samples with all 12 bands\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def resize_band(self, band_data, target_shape):\n",
    "        if band_data.shape == target_shape:\n",
    "            return band_data\n",
    "        \n",
    "        resized = cv2.resize(\n",
    "            band_data, \n",
    "            (target_shape[1], target_shape[0]),  # OpenCV uses (W, H)\n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        return resized\n",
    "    \n",
    "    def load_multispectral_image(self, band_paths):\n",
    "        bands_data = []\n",
    "        shapes = []\n",
    "        \n",
    "        loaded_bands = {}\n",
    "        for band in self.bands:\n",
    "            img = tifffile.imread(band_paths[band])\n",
    "            loaded_bands[band] = img\n",
    "            shapes.append(img.shape)\n",
    "        \n",
    "        if self.target_size is not None:\n",
    "            target_shape = self.target_size\n",
    "        else:\n",
    "            reference_band = loaded_bands['B4']  # 10m band\n",
    "            target_shape = reference_band.shape\n",
    "        \n",
    "        # Second pass: resize all bands to target shape\n",
    "        for band in self.bands:\n",
    "            img = loaded_bands[band]\n",
    "            \n",
    "            # Resize if needed\n",
    "            if img.shape != target_shape:\n",
    "                img = self.resize_band(img, target_shape)\n",
    "            \n",
    "            bands_data.append(img)\n",
    "        \n",
    "        # Stack along channel dimension: (12, H, W)\n",
    "        multi_band = np.stack(bands_data, axis=0)\n",
    "        return multi_band \n",
    "    \n",
    "    def normalize_sentinel2(self, img):\n",
    "        \"\"\"\n",
    "        Normalize Sentinel-2 data to [0, 1]\n",
    "        Sentinel-2 L1C has typical range 0-10000 (reflectance * 10000)\n",
    "        \"\"\"\n",
    "        # Clip extreme values and normalize\n",
    "        img = np.clip(img, 0, 10000)\n",
    "        img = img.astype(np.float32) / 10000.0\n",
    "        return img  \n",
    "    \n",
    "    def normalize_satlas(self,img):\n",
    "        img = np.clip(img, 0, 8160)\n",
    "        img = img.astype(np.float32) / 8160\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        band_paths,label = self.samples[idx]\n",
    "        img = self.load_multispectral_image(band_paths)\n",
    "        img = self.normalize_satlas(img)\n",
    "        if(label == \"RPH\"): label = 0\n",
    "        elif(label == \"Blast\"): label = 1\n",
    "        elif(label == \"Rust\"): label = 2\n",
    "        elif(label == \"Aphid\"): label = 3\n",
    "        self.bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
    "        c9 = img[(3,2,1,4,5,6,7,10,11),:,:]\n",
    "        c12 = img\n",
    "        return {\n",
    "            'c9': c9,      \n",
    "            'c12': c12,\n",
    "            'label': label\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137280e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 samples with all 12 bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cdata = ClassificationDataset(\n",
    "    root_dir=\"C:\\\\Users\\\\ayush\\\\OneDrive\\\\Desktop\\\\Agriculture\\\\ICPR02\\\\kaggle\",\n",
    "    mask_type='random',\n",
    "    augment=True,\n",
    "    target_size=(256, 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5606a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "\n",
    "def create_balanced_split(dataset, val_samples_per_class=None, max_train_per_class=None):\n",
    "    \"\"\"\n",
    "    Create balanced train/val split with configurable number of samples per class\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your ClassificationDataset\n",
    "        val_samples_per_class: int or dict mapping class_id to number of val samples\n",
    "                              If int, same number for all classes\n",
    "                              If dict, e.g., {0: 20, 1: 15, 2: 25, 3: 20}\n",
    "        max_train_per_class: int or dict mapping class_id to max training samples\n",
    "                            If int, same number for all classes\n",
    "                            If dict, e.g., {0: 100, 1: 80, 2: 120, 3: 100}\n",
    "    \n",
    "    Returns:\n",
    "        train_indices, val_indices\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    if val_samples_per_class is None:\n",
    "        val_samples_per_class = 20\n",
    "    if max_train_per_class is None:\n",
    "        max_train_per_class = 100\n",
    "    \n",
    "    # Convert to dict if int is provided\n",
    "    if isinstance(val_samples_per_class, int):\n",
    "        val_samples_per_class = {0: val_samples_per_class, 1: val_samples_per_class, \n",
    "                                 2: val_samples_per_class, 3: val_samples_per_class}\n",
    "    \n",
    "    if isinstance(max_train_per_class, int):\n",
    "        max_train_per_class = {0: max_train_per_class, 1: max_train_per_class, \n",
    "                               2: max_train_per_class, 3: max_train_per_class}\n",
    "    \n",
    "    # Get all labels\n",
    "    all_labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        all_labels.append(sample['label'])\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_indices = {\n",
    "        0: np.where(all_labels == 0)[0],  # RPH\n",
    "        1: np.where(all_labels == 1)[0],  # Blast\n",
    "        2: np.where(all_labels == 2)[0],  # Rust\n",
    "        3: np.where(all_labels == 3)[0],  # Aphid\n",
    "    }\n",
    "    \n",
    "    # Print original class distribution\n",
    "    print(\"Original class distribution:\")\n",
    "    class_names = ['RPH', 'Blast', 'Rust', 'Aphid']\n",
    "    for cls_id, indices in class_indices.items():\n",
    "        print(f\"  {class_names[cls_id]}: {len(indices)} samples\")\n",
    "    \n",
    "    train_indices = {}\n",
    "    val_indices = {}\n",
    "    \n",
    "    print(f\"\\nSplitting with custom samples per class:\")\n",
    "    \n",
    "    # For each class, split into train/val\n",
    "    for cls_id, indices in class_indices.items():\n",
    "        # Get the specific counts for this class\n",
    "        val_count = val_samples_per_class[cls_id]\n",
    "        train_max = max_train_per_class[cls_id]\n",
    "        \n",
    "        # Shuffle indices for this class\n",
    "        shuffled = indices.copy()\n",
    "        np.random.shuffle(shuffled)\n",
    "        \n",
    "        # Take first val_count for validation\n",
    "        val_idx = shuffled[:val_count]\n",
    "        \n",
    "        # Take next train_max for training\n",
    "        remaining = shuffled[val_count:]\n",
    "        train_idx = remaining[:train_max]\n",
    "        \n",
    "        # Calculate how many were left out\n",
    "        left_out = len(remaining) - len(train_idx)\n",
    "        \n",
    "        val_indices[cls_id] = val_idx\n",
    "        train_indices[cls_id] = train_idx\n",
    "        \n",
    "        print(f\"  {class_names[cls_id]}: {len(train_idx)} train (max: {train_max}), \"\n",
    "              f\"{len(val_idx)} val (target: {val_count}), {left_out} left out\")\n",
    "    \n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b44fd6",
   "metadata": {},
   "source": [
    "# 9 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_classification_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir='satlas_baseline_classification_checkpoints9',\n",
    "    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0]),\n",
    "    minus = 2\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Freeze encoder (optional - remove these lines if you want to fine-tune)\n",
    "    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    # Tracking\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ================== TRAINING ==================\n",
    "        # Keep encoder in eval mode if frozen\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in pbar:\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'] - minus\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100. * correct / total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # ================== VALIDATION ==================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in pbar:\n",
    "                images = batch['c9'].to(device)\n",
    "                labels = batch['label'] - minus\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100. * val_correct / val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(save_dir, 'best_classifier.pth'))\n",
    "            print(f'  âœ“ Saved best model with val_acc = {val_acc:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nðŸŽ‰ Training complete!')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db2489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device='cuda', class_names=['RPH', 'Blast', 'Rust', 'Aphid'], save_path=None,minus = 0):\n",
    "    \"\"\"\n",
    "    Evaluate model and display classification report and confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate\n",
    "        dataloader: DataLoader with test/validation data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        class_names: List of class names for display\n",
    "        save_path: Optional path to save confusion matrix image\n",
    "    \n",
    "    Returns:\n",
    "        dict with predictions, labels, accuracy, and confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth_labels = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            labels = labels - minus\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truth_labels = np.array(ground_truth_labels)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = 100. * np.sum(predictions == ground_truth_labels) / len(ground_truth_labels)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(ground_truth_labels, predictions, \n",
    "                                target_names=class_names, \n",
    "                                digits=4))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(ground_truth_labels, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix\\nOverall Accuracy: {accuracy:.2f}%', fontsize=14)\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            total = cm[i].sum()\n",
    "            if total > 0:\n",
    "                percentage = cm[i, j] / total * 100\n",
    "                plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                        ha='center', va='center', fontsize=9, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Confusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total Samples: {len(ground_truth_labels)}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'labels': ground_truth_labels,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b6c8e",
   "metadata": {},
   "source": [
    "# DataSet Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f49aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "  RPH: 495 samples\n",
      "  Blast: 75 samples\n",
      "  Rust: 40 samples\n",
      "  Aphid: 290 samples\n",
      "\n",
      "Splitting with custom samples per class:\n",
      "  RPH: 100 train (max: 100), 20 val (target: 20), 375 left out\n",
      "  Blast: 55 train (max: 100), 20 val (target: 20), 0 left out\n",
      "  Rust: 20 train (max: 100), 20 val (target: 20), 0 left out\n",
      "  Aphid: 100 train (max: 100), 20 val (target: 20), 170 left out\n"
     ]
    }
   ],
   "source": [
    "train_indices, val_indices = create_balanced_split(\n",
    "    cdata, \n",
    "    val_samples_per_class={0:20, 1:20, 2:20, 3:20},\n",
    "    max_train_per_class= {0:100, 1:100, 2:100, 3:100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27951a27",
   "metadata": {},
   "source": [
    "# RPH BLAST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778e8e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoaders created successfully!\n",
      "Train batches: 10\n",
      "Val batches: 3\n"
     ]
    }
   ],
   "source": [
    "train_dataset1 = Subset(cdata, np.concat((train_indices[0],train_indices[1]),axis=0))\n",
    "val_dataset1 = Subset(cdata, np.concat((val_indices[0],val_indices[1]),axis=0))\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader1)}\")\n",
    "print(f\"Val batches: {len(val_loader1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda1717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel1(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256 * 8 * 8,256 * 6),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256 * 6,128 * 3),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(128 * 3,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(64,2),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2af0996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 113,987,874\n",
      "Trainable parameters: 113,987,874\n",
      "Frozen parameters: 0\n",
      "Training on cuda\n",
      "Total trainable parameters: 113,987,874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.08s/it, loss=0.4520, acc=40.00%]\n",
      "Epoch 1/20 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.66it/s, loss=1.9328, acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20:\n",
      "  Train Loss: 0.7225 | Train Acc: 40.00%\n",
      "  Val Loss:   1.2507 | Val Acc:   50.00%\n",
      "  âœ“ Saved best model with val_acc = 50.00%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:23<00:00,  2.33s/it, loss=0.6945, acc=48.39%]\n",
      "Epoch 2/20 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.77it/s, loss=0.6429, acc=50.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20:\n",
      "  Train Loss: 0.8114 | Train Acc: 48.39%\n",
      "  Val Loss:   0.6865 | Val Acc:   50.00%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:13<00:19,  3.26s/it, loss=0.6611, acc=40.62%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozen parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrozen_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_classification_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrph_blast_classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m155.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m155.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m55.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m, in \u001b[0;36mtrain_classification_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, device, save_dir, class_weights, minus)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1 = ClassificationModel1()\n",
    "\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "trainable_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model1,\n",
    "    train_loader=train_loader1,\n",
    "    val_loader=val_loader1,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-4 * 2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir=\"rph_blast_classifier\",\n",
    "    class_weights = torch.tensor([155.0/100.0,155.0/55.0]),\n",
    "    minus= 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel1(None)\n",
    "best_model.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['RPH', 'Blast'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ab414",
   "metadata": {},
   "source": [
    "# Rust Aphid MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099313d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2 = Subset(cdata, np.concat((train_indices[2],train_indices[3]),axis=0))\n",
    "val_dataset2 = Subset(cdata, np.concat((val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset2,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader2)}\")\n",
    "print(f\"Val batches: {len(val_loader2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel2(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256 * 8 * 8,256 * 6),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256 * 6,128 * 3),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(128 * 3,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(64,2),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b5b1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassificationModel2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationModel2\u001b[49m()\n\u001b[0;32m      3\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model2\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m      4\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model2\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ClassificationModel2' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = ClassificationModel2()\n",
    "\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "trainable_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model2,\n",
    "    train_loader=train_loader2,\n",
    "    val_loader=val_loader2,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-4 * 2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir=\"rust_aphid_classifier\",\n",
    "    class_weights = torch.tensor([170/150.0,170/20.0]),\n",
    "    minus=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a706f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel2(None)\n",
    "best_model.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['Rust', 'Aphid'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png',\n",
    "    minus=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5770fb",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(cdata, np.concat((train_indices[0],train_indices[1],train_indices[2],train_indices[3]),axis=0))\n",
    "val_dataset = Subset(cdata, np.concat((val_indices[0],val_indices[1],val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256 * 8 * 8,256 * 8),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256 * 8,256),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(64,4),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-4 * 5,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    minus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel(None)\n",
    "best_model.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['RPH', 'Blast', 'Rust', 'Aphid'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59223d4",
   "metadata": {},
   "source": [
    "# ENSEMBLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasete = Subset(cdata, np.concat((train_indices[0],train_indices[1],train_indices[2],train_indices[3]),axis=0))\n",
    "val_datasete = Subset(cdata, np.concat((val_indices[0],val_indices[1],val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loadere = DataLoader(\n",
    "    train_datasete,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loadere = DataLoader(\n",
    "    val_datasete,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loadere)}\")\n",
    "print(f\"Val batches: {len(val_loadere)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble3model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.div = ClassificationModel(None)\n",
    "        self.div.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "        self.m1 = ClassificationModel1(None)\n",
    "        self.m1.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m1l = nn.Linear(4,2)\n",
    "\n",
    "        self.m2 = ClassificationModel2(None)\n",
    "        self.m2.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m2l = nn.Linear(4,2)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "\n",
    "        # Get predictions from the division model\n",
    "        x = self.div(img)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        # Determine which samples go to which model\n",
    "        # mask is True where x[0] + x[1] > x[2] + x[3]\n",
    "        mask = (x[:, 0] + x[:, 1]) > (x[:, 2] + x[:, 3])\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, 4, device=img.device, dtype=x.dtype)\n",
    "\n",
    "        # Process samples that go to model 1\n",
    "        if mask.any():\n",
    "            indices_m1 = mask.nonzero(as_tuple=True)[0]\n",
    "            y1 = self.m1(img[indices_m1])\n",
    "            \n",
    "\n",
    "            # Combine features for m1l\n",
    "            combined_m1 = torch.cat([\n",
    "                x[indices_m1, :2],  # x[0], x[1]\n",
    "                y1[:, :2]            # y[0], y[1]\n",
    "            ], dim=1)\n",
    "\n",
    "            o1 = self.m1l(combined_m1)\n",
    "            o1 = self.softmax(o1)\n",
    "            output[indices_m1, :2] = o1\n",
    "\n",
    "        # Process samples that go to model 2\n",
    "        if (~mask).any():\n",
    "            indices_m2 = (~mask).nonzero(as_tuple=True)[0]\n",
    "            y2 = self.m2(img[indices_m2])\n",
    "           \n",
    "\n",
    "            # Combine features for m2l\n",
    "            combined_m2 = torch.cat([\n",
    "                x[indices_m2, 2:],  # x[2], x[3]\n",
    "                y2[:, :2]            # y[0], y[1]\n",
    "            ], dim=1)\n",
    "\n",
    "            o2 = self.m2l(combined_m2)\n",
    "            o2 = self.softmax(o2)\n",
    "            output[indices_m2, 2:] = o2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945399a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_ensemble_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir='satlas_ensemble_classification_checkpoints9',\n",
    "    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0]),\n",
    "    minus = 2\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    # Tracking\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # ================== TRAINING ==================\n",
    "        # Keep encoder in eval mode if frozen\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in pbar:\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'] - minus\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100. * correct / total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # ================== VALIDATION ==================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in pbar:\n",
    "                images = batch['c9'].to(device)\n",
    "                labels = batch['label'] - minus\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100. * val_correct / val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(save_dir, 'best_classifier.pth'))\n",
    "            print(f'  âœ“ Saved best model with val_acc = {val_acc:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nðŸŽ‰ Training complete!')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06da451",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = ensemble3model()\n",
    "\n",
    "for param in modele.div.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for param in modele.m1.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for param in modele.m2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "modele.div.eval()\n",
    "modele.m1.eval()\n",
    "modele.m2.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in modele.parameters())\n",
    "trainable_params = sum(p.numel() for p in modele.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_ensemble_model(\n",
    "    model=modele,\n",
    "    train_loader=train_loadere,\n",
    "    val_loader=val_loadere,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3 * 5,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    minus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff871b2",
   "metadata": {},
   "source": [
    "# Ensemble if else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2412557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble3model_ifelse(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.div = ClassificationModel(None)\n",
    "        self.div.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "        self.m1 = ClassificationModel1(None)\n",
    "        self.m1.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m1l = nn.Linear(4,2)\n",
    "\n",
    "        self.m2 = ClassificationModel2(None)\n",
    "        self.m2.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m2l = nn.Linear(4,2)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "\n",
    "        # Get predictions from the division model\n",
    "        x = self.div(img)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        # Determine which samples go to which model\n",
    "        # mask is True where x[0] + x[1] > x[2] + x[3]\n",
    "        mask = (x[:, 0] + x[:, 1]) > (x[:, 2] + x[:, 3])\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, 4, device=img.device, dtype=x.dtype)\n",
    "\n",
    "        # Process samples that go to model 1\n",
    "        if mask.any():\n",
    "            indices_m1 = mask.nonzero(as_tuple=True)[0]\n",
    "            y1 = self.m1(img[indices_m1])\n",
    "            y1 = self.softmax(y1)\n",
    "        \n",
    "            output[indices_m1, :2] = y1\n",
    "\n",
    "        # Process samples that go to model 2\n",
    "        if (~mask).any():\n",
    "            indices_m2 = (~mask).nonzero(as_tuple=True)[0]\n",
    "            y2 = self.m2(img[indices_m2])\n",
    "            y2 = self.softmax(y2)\n",
    "\n",
    "            output[indices_m2, 2:] = y2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb664f42",
   "metadata": {},
   "source": [
    "# Submission Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286998bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from eval import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_eval = EvalDataset(\"..\\\\ICPR02\\\\kaggle\",target_size=(256, 256))\n",
    "modele = ensemble3model().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    for i in range(len(data_eval)):\n",
    "        input = (torch.from_numpy(data_eval.__getitem__(i)['c9']).to(device)).unsqueeze(0)\n",
    "        outputs.append(torch.argmax(modele(input)).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval.write_csv(outputs,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbd61",
   "metadata": {},
   "source": [
    "# Submission Ensemble if else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499101cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from eval import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_eval = EvalDataset(\"..\\\\ICPR02\\\\kaggle\",target_size=(256, 256))\n",
    "modele = ensemble3model_ifelse().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    for i in range(len(data_eval)):\n",
    "        input = (torch.from_numpy(data_eval.__getitem__(i)['c9']).to(device)).unsqueeze(0)\n",
    "        outputs.append(torch.argmax(modele(input)).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval.write_csv(outputs,\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
