{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from classification_datasets import ClassificationDataset, ClassificationTrainDataset, ClassificationValDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137280e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = ClassificationDataset(\n",
    "    root_dir=\"../kaggle\",\n",
    "    mask_type='random',\n",
    "    augment=True,\n",
    "    target_size=(256, 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def create_balanced_split(dataset, val_samples_per_class=None, max_train_per_class=None):\n",
    "    \"\"\"\n",
    "    Create balanced train/val split with configurable number of samples per class\n",
    "    \n",
    "    Args:\n",
    "        dataset: Your ClassificationDataset\n",
    "        val_samples_per_class: int or dict mapping class_id to number of val samples\n",
    "                              If int, same number for all classes\n",
    "                              If dict, e.g., {0: 20, 1: 15, 2: 25, 3: 20}\n",
    "        max_train_per_class: int or dict mapping class_id to max training samples\n",
    "                            If int, same number for all classes\n",
    "                            If dict, e.g., {0: 100, 1: 80, 2: 120, 3: 100}\n",
    "    \n",
    "    Returns:\n",
    "        train_indices, val_indices\n",
    "    \"\"\"\n",
    "    # Default values\n",
    "    if val_samples_per_class is None:\n",
    "        val_samples_per_class = 20\n",
    "    if max_train_per_class is None:\n",
    "        max_train_per_class = 100\n",
    "    \n",
    "    # Convert to dict if int is provided\n",
    "    if isinstance(val_samples_per_class, int):\n",
    "        val_samples_per_class = {0: val_samples_per_class, 1: val_samples_per_class, \n",
    "                                 2: val_samples_per_class, 3: val_samples_per_class}\n",
    "    \n",
    "    if isinstance(max_train_per_class, int):\n",
    "        max_train_per_class = {0: max_train_per_class, 1: max_train_per_class, \n",
    "                               2: max_train_per_class, 3: max_train_per_class}\n",
    "    \n",
    "    # Get all labels\n",
    "    all_labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        sample = dataset[idx]\n",
    "        all_labels.append(sample['label'])\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_indices = {\n",
    "        0: np.where(all_labels == 0)[0],  # RPH\n",
    "        1: np.where(all_labels == 1)[0],  # Blast\n",
    "        2: np.where(all_labels == 2)[0],  # Rust\n",
    "        3: np.where(all_labels == 3)[0],  # Aphid\n",
    "    }\n",
    "    \n",
    "    # Print original class distribution\n",
    "    print(\"Original class distribution:\")\n",
    "    class_names = ['RPH', 'Blast', 'Rust', 'Aphid']\n",
    "    for cls_id, indices in class_indices.items():\n",
    "        print(f\"  {class_names[cls_id]}: {len(indices)} samples\")\n",
    "    \n",
    "    train_indices = {}\n",
    "    val_indices = {}\n",
    "    \n",
    "    print(f\"\\nSplitting with custom samples per class:\")\n",
    "    \n",
    "    # For each class, split into train/val\n",
    "    for cls_id, indices in class_indices.items():\n",
    "        # Get the specific counts for this class\n",
    "        val_count = val_samples_per_class[cls_id]\n",
    "        train_max = max_train_per_class[cls_id]\n",
    "        \n",
    "        # Shuffle indices for this class\n",
    "        shuffled = indices.copy()\n",
    "        np.random.shuffle(shuffled)\n",
    "        \n",
    "        # Take first val_count for validation\n",
    "        val_idx = shuffled[:val_count]\n",
    "        \n",
    "        # Take next train_max for training\n",
    "        remaining = shuffled[val_count:]\n",
    "        train_idx = remaining[:train_max]\n",
    "        \n",
    "        # Calculate how many were left out\n",
    "        left_out = len(remaining) - len(train_idx)\n",
    "        \n",
    "        val_indices[cls_id] = val_idx\n",
    "        train_indices[cls_id] = train_idx\n",
    "        \n",
    "        print(f\"  {class_names[cls_id]}: {len(train_idx)} train (max: {train_max}), \"\n",
    "              f\"{len(val_idx)} val (target: {val_count}), {left_out} left out\")\n",
    "    \n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b44fd6",
   "metadata": {},
   "source": [
    "# 9 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_classification_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir='satlas_baseline_classification_checkpoints9',\n",
    "    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0]),\n",
    "    minus = 2\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Freeze encoder (optional - remove these lines if you want to fine-tune)\n",
    "    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    # Tracking\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # ================== TRAINING ==================\n",
    "        # Keep encoder in eval mode if frozen\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in pbar:\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'] - minus\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100. * correct / total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # ================== VALIDATION ==================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in pbar:\n",
    "                images = batch['c9'].to(device)\n",
    "                labels = batch['label'] - minus\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100. * val_correct / val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(save_dir, 'best_classifier.pth'))\n",
    "            print(f'  âœ“ Saved best model with val_acc = {val_acc:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nðŸŽ‰ Training complete!')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device='cuda', class_names=['RPH', 'Blast', 'Rust', 'Aphid'], save_path=None,minus = 0):\n",
    "    \"\"\"\n",
    "    Evaluate model and display classification report and confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate\n",
    "        dataloader: DataLoader with test/validation data\n",
    "        device: 'cuda' or 'cpu'\n",
    "        class_names: List of class names for display\n",
    "        save_path: Optional path to save confusion matrix image\n",
    "    \n",
    "    Returns:\n",
    "        dict with predictions, labels, accuracy, and confusion matrix\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth_labels = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            labels = labels - minus\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            ground_truth_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truth_labels = np.array(ground_truth_labels)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = 100. * np.sum(predictions == ground_truth_labels) / len(ground_truth_labels)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(ground_truth_labels, predictions, \n",
    "                                target_names=class_names, \n",
    "                                digits=4))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(ground_truth_labels, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.title(f'Confusion Matrix\\nOverall Accuracy: {accuracy:.2f}%', fontsize=14)\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            total = cm[i].sum()\n",
    "            if total > 0:\n",
    "                percentage = cm[i, j] / total * 100\n",
    "                plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                        ha='center', va='center', fontsize=9, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nâœ“ Confusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Total Samples: {len(ground_truth_labels)}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'labels': ground_truth_labels,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b6c8e",
   "metadata": {},
   "source": [
    "# DataSet Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f49aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = create_balanced_split(\n",
    "    cdata, \n",
    "    val_samples_per_class={0:20, 1:20, 2:20, 3:20},\n",
    "    max_train_per_class= {0:100, 1:100, 2:100, 3:100}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27951a27",
   "metadata": {},
   "source": [
    "# RPH BLAST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1 = Subset(cdata, np.concat((train_indices[0],train_indices[1]),axis=0))\n",
    "val_dataset1 = Subset(cdata, np.concat((val_indices[0],val_indices[1]),axis=0))\n",
    "\n",
    "train_loader1 = DataLoader(\n",
    "    train_dataset1,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    val_dataset1,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader1)}\")\n",
    "print(f\"Val batches: {len(val_loader1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel1(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                     \n",
    "                        nn.Linear(256 * 8 * 8,256 * 6),\n",
    "                        nn.LeakyReLU(),\n",
    "                        \n",
    "                        nn.Linear(256 * 6,128 * 3),\n",
    "                        nn.LeakyReLU(),\n",
    "                      \n",
    "                        nn.Linear(128 * 3,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                       \n",
    "                        nn.Linear(64,2),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ClassificationModel1()\n",
    "\n",
    "total_params = sum(p.numel() for p in model1.parameters())\n",
    "trainable_params = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model1,\n",
    "    train_loader=train_loader1,\n",
    "    val_loader=val_loader1,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3 ,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir=\"rph_blast_classifier\",\n",
    "    class_weights = torch.tensor([155.0/100.0,155.0/55.0]),\n",
    "    minus= 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bc69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel1(None)\n",
    "best_model.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader1,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['RPH', 'Blast'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ab414",
   "metadata": {},
   "source": [
    "# Rust Aphid MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099313d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset2 = Subset(cdata, np.concat((train_indices[2],train_indices[3]),axis=0))\n",
    "val_dataset2 = Subset(cdata, np.concat((val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "train_loader2 = DataLoader(\n",
    "    train_dataset2,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader2 = DataLoader(\n",
    "    val_dataset2,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader2)}\")\n",
    "print(f\"Val batches: {len(val_loader2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel2(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256 * 8 * 8,256 * 6),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256 * 6,128 * 3),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(128 * 3,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(64,2),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b5b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ClassificationModel2()\n",
    "\n",
    "total_params = sum(p.numel() for p in model2.parameters())\n",
    "trainable_params = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model2,\n",
    "    train_loader=train_loader2,\n",
    "    val_loader=val_loader2,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-4 * 2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir=\"rust_aphid_classifier\",\n",
    "    class_weights = torch.tensor([170/150.0,170/20.0]),\n",
    "    minus=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a706f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel2(None)\n",
    "best_model.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['Rust', 'Aphid'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png',\n",
    "    minus=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5770fb",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(cdata, np.concat((train_indices[0],train_indices[1],train_indices[2],train_indices[3]),axis=0))\n",
    "val_dataset = Subset(cdata, np.concat((val_indices[0],val_indices[1],val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from satlasswin import SatlasSwin\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, encoder_path = None, *args, **kwargs ,):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = SatlasSwin(channels=9)\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.encoder.eval()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "                        nn.Conv2d(1024,256,kernel_size=1,stride=1),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(256 * 8 * 8,256 * 8),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256 * 8,256),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(256,64),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(64,4),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = x[3]\n",
    "        return self.stack(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_classification_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-4 * 5,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    minus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, load best model and evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON BEST MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = ClassificationModel(None)\n",
    "best_model.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "final_results = evaluate_model(\n",
    "    model=best_model,\n",
    "    dataloader=val_loader,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names=['RPH', 'Blast', 'Rust', 'Aphid'],\n",
    "    save_path='satlas_baseline_classification_checkpoints9/final_confusion_matrix.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59223d4",
   "metadata": {},
   "source": [
    "# ENSEMBLE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd4379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasete = Subset(cdata, np.concat((train_indices[0],train_indices[1],train_indices[2],train_indices[3]),axis=0))\n",
    "val_datasete = Subset(cdata, np.concat((val_indices[0],val_indices[1],val_indices[2],val_indices[3]),axis=0))\n",
    "\n",
    "# Create dataloaders\n",
    "train_loadere = DataLoader(\n",
    "    train_datasete,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loadere = DataLoader(\n",
    "    val_datasete,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created successfully!\")\n",
    "print(f\"Train batches: {len(train_loadere)}\")\n",
    "print(f\"Val batches: {len(val_loadere)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble3model(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.div = ClassificationModel(None)\n",
    "        self.div.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "        self.m1 = ClassificationModel1(None)\n",
    "        self.m1.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m1l = nn.Linear(4,2)\n",
    "\n",
    "        self.m2 = ClassificationModel2(None)\n",
    "        self.m2.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m2l = nn.Linear(4,2)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "\n",
    "        # Get predictions from the division model\n",
    "        x = self.div(img)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        # Determine which samples go to which model\n",
    "        # mask is True where x[0] + x[1] > x[2] + x[3]\n",
    "        mask = (x[:, 0] + x[:, 1]) > (x[:, 2] + x[:, 3])\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, 4, device=img.device, dtype=x.dtype)\n",
    "\n",
    "        # Process samples that go to model 1\n",
    "        if mask.any():\n",
    "            indices_m1 = mask.nonzero(as_tuple=True)[0]\n",
    "            y1 = self.m1(img[indices_m1])\n",
    "            \n",
    "\n",
    "            # Combine features for m1l\n",
    "            combined_m1 = torch.cat([\n",
    "                x[indices_m1, :2],  # x[0], x[1]\n",
    "                y1[:, :2]            # y[0], y[1]\n",
    "            ], dim=1)\n",
    "\n",
    "            o1 = self.m1l(combined_m1)\n",
    "            o1 = self.softmax(o1)\n",
    "            output[indices_m1, :2] = o1\n",
    "\n",
    "        # Process samples that go to model 2\n",
    "        if (~mask).any():\n",
    "            indices_m2 = (~mask).nonzero(as_tuple=True)[0]\n",
    "            y2 = self.m2(img[indices_m2])\n",
    "           \n",
    "\n",
    "            # Combine features for m2l\n",
    "            combined_m2 = torch.cat([\n",
    "                x[indices_m2, 2:],  # x[2], x[3]\n",
    "                y2[:, :2]            # y[0], y[1]\n",
    "            ], dim=1)\n",
    "\n",
    "            o2 = self.m2l(combined_m2)\n",
    "            o2 = self.softmax(o2)\n",
    "            output[indices_m2, 2:] = o2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945399a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_ensemble_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir='satlas_ensemble_classification_checkpoints9',\n",
    "    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0]),\n",
    "    minus = 2\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)    \n",
    "    \n",
    "    # Loss and optimizer\n",
    "    class_weights = class_weights.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    # Tracking\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # ================== TRAINING ==================\n",
    "        # Keep encoder in eval mode if frozen\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        for batch in pbar:\n",
    "            images = batch['c9'].to(device)\n",
    "            labels = batch['label'] - minus\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100. * correct / total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # ================== VALIDATION ==================\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "            for batch in pbar:\n",
    "                images = batch['c9'].to(device)\n",
    "                labels = batch['label'] - minus\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100. * val_correct / val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "            }, os.path.join(save_dir, 'best_classifier.pth'))\n",
    "            print(f'  âœ“ Saved best model with val_acc = {val_acc:.2f}%')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accs, label='Train Acc', marker='o')\n",
    "    ax2.plot(val_accs, label='Val Acc', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nðŸŽ‰ Training complete!')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06da451",
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = ensemble3model()\n",
    "\n",
    "for param in modele.div.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for param in modele.m1.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for param in modele.m2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "modele.div.eval()\n",
    "modele.m1.eval()\n",
    "modele.m2.eval()\n",
    "\n",
    "total_params = sum(p.numel() for p in modele.parameters())\n",
    "trainable_params = sum(p.numel() for p in modele.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "\n",
    "trained_model, history = train_ensemble_model(\n",
    "    model=modele,\n",
    "    train_loader=train_loadere,\n",
    "    val_loader=val_loadere,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3 * 5,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    minus=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff871b2",
   "metadata": {},
   "source": [
    "# Ensemble if else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2412557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble3model_ifelse(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.div = ClassificationModel(None)\n",
    "        self.div.load_state_dict(torch.load(\"satlas_baseline_classification_checkpoints9/best_classifier.pth\")[\"model_state_dict\"])\n",
    "\n",
    "        self.m1 = ClassificationModel1(None)\n",
    "        self.m1.load_state_dict(torch.load(\"rph_blast_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m1l = nn.Linear(4,2)\n",
    "\n",
    "        self.m2 = ClassificationModel2(None)\n",
    "        self.m2.load_state_dict(torch.load(\"rust_aphid_classifier/best_classifier.pth\")[\"model_state_dict\"])\n",
    "        self.m2l = nn.Linear(4,2)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "\n",
    "        # Get predictions from the division model\n",
    "        x = self.div(img)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        # Determine which samples go to which model\n",
    "        # mask is True where x[0] + x[1] > x[2] + x[3]\n",
    "        mask = (x[:, 0] + x[:, 1]) > (x[:, 2] + x[:, 3])\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, 4, device=img.device, dtype=x.dtype)\n",
    "\n",
    "        # Process samples that go to model 1\n",
    "        if mask.any():\n",
    "            indices_m1 = mask.nonzero(as_tuple=True)[0]\n",
    "            y1 = self.m1(img[indices_m1])\n",
    "            y1 = self.softmax(y1)\n",
    "        \n",
    "            output[indices_m1, :2] = y1\n",
    "\n",
    "        # Process samples that go to model 2\n",
    "        if (~mask).any():\n",
    "            indices_m2 = (~mask).nonzero(as_tuple=True)[0]\n",
    "            y2 = self.m2(img[indices_m2])\n",
    "            y2 = self.softmax(y2)\n",
    "\n",
    "            output[indices_m2, 2:] = y2\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb664f42",
   "metadata": {},
   "source": [
    "# Submission Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286998bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from eval import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_eval = EvalDataset(\"..\\\\ICPR02\\\\kaggle\",target_size=(256, 256))\n",
    "modele = ensemble3model().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    for i in range(len(data_eval)):\n",
    "        input = (torch.from_numpy(data_eval.__getitem__(i)['c9']).to(device)).unsqueeze(0)\n",
    "        outputs.append(torch.argmax(modele(input)).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval.write_csv(outputs,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbd61",
   "metadata": {},
   "source": [
    "# Submission Ensemble if else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499101cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append('../')\n",
    "from eval import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_eval = EvalDataset(\"..\\\\ICPR02\\\\kaggle\",target_size=(256, 256))\n",
    "modele = ensemble3model_ifelse().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    for i in range(len(data_eval)):\n",
    "        input = (torch.from_numpy(data_eval.__getitem__(i)['c9']).to(device)).unsqueeze(0)\n",
    "        outputs.append(torch.argmax(modele(input)).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval.write_csv(outputs,\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
