{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b797729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "from dataset import Sentinel2InpaintingDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a115e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 664/4640 [00:02<00:13, 288.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 samples with all 12 bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_root='../s2a.tar/s2a'\n",
    "\n",
    "# In train.py, use target_size that's divisible by 32:\n",
    "dataset = Sentinel2InpaintingDataset(\n",
    "    root_dir=data_root,\n",
    "    mask_type='random',\n",
    "    limit_samples= 4000,\n",
    "    target_size=(256, 256),  # or (256, 256), (384, 384), etc.\n",
    "    format=\"satlas\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751199b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6179d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InfoNCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    InfoNCE contrastive loss for SASSL.\n",
    "    Maximizes agreement between student views and teacher view of same image,\n",
    "    while minimizing agreement with other images in the batch (negatives).\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, teacher_output, student_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            teacher_output: (batch_size, embed_dim) - teacher embeddings\n",
    "            student_outputs: list of (batch_size, embed_dim) - student embeddings\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        batch_size = teacher_output.shape[0]\n",
    "        \n",
    "        # Normalize embeddings (L2 normalization)\n",
    "        teacher_output = F.normalize(teacher_output, dim=-1, p=2)\n",
    "        student_outputs = [F.normalize(s, dim=-1, p=2) for s in student_outputs]\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Compute loss for each student view\n",
    "        for student_output in student_outputs:\n",
    "            # Compute similarity matrix: (batch_size, batch_size)\n",
    "            # logits[i, j] = cosine similarity between student_i and teacher_j\n",
    "            logits = torch.matmul(student_output, teacher_output.T) / self.temperature\n",
    "            \n",
    "            # Labels: diagonal elements are positives (same image index)\n",
    "            # Off-diagonal elements are negatives (different images)\n",
    "            labels = torch.arange(batch_size, device=logits.device)\n",
    "            \n",
    "            # Cross-entropy loss\n",
    "            # This maximizes logits[i, i] and minimizes logits[i, j] for i ≠ j\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Average over all student views (4 local + 1 spectral = 5 views)\n",
    "        return total_loss / len(student_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10574d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 177,476,608\n",
      "Trainable parameters: 88,738,304\n",
      "Non-trainable parameters: 88,738,304\n",
      "Percentage trainable: 50.00%\n",
      "\n",
      "--- Student Model ---\n",
      "Student total: 87,943,136\n",
      "Student trainable: 87,943,136\n",
      "\n",
      "--- Teacher Model ---\n",
      "Teacher total: 87,943,136\n",
      "Teacher trainable: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 2000/2000 [27:33<00:00,  1.21it/s, loss=0.6911, avg_loss=0.7601, lr=0.000100, momentum=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 Summary:\n",
      "  Average Loss: 0.7601\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 2000/2000 [27:20<00:00,  1.22it/s, loss=0.9797, avg_loss=0.8080, lr=0.000100, momentum=0.1992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100 Summary:\n",
      "  Average Loss: 0.8080\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 2000/2000 [27:32<00:00,  1.21it/s, loss=2.7423, avg_loss=0.8384, lr=0.000100, momentum=0.3984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100 Summary:\n",
      "  Average Loss: 0.8384\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 2000/2000 [27:23<00:00,  1.22it/s, loss=0.3928, avg_loss=0.9189, lr=0.000100, momentum=0.5976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100 Summary:\n",
      "  Average Loss: 0.9189\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "✓ Checkpoint saved: sassl_checkpoint_epoch_4.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 2000/2000 [27:27<00:00,  1.21it/s, loss=0.6898, avg_loss=0.8256, lr=0.000100, momentum=0.7968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100 Summary:\n",
      "  Average Loss: 0.8256\n",
      "  Learning Rate: 0.000099\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 2000/2000 [27:24<00:00,  1.22it/s, loss=0.1350, avg_loss=0.6724, lr=0.000099, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100 Summary:\n",
      "  Average Loss: 0.6724\n",
      "  Learning Rate: 0.000099\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 2000/2000 [27:22<00:00,  1.22it/s, loss=0.2968, avg_loss=0.7095, lr=0.000099, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100 Summary:\n",
      "  Average Loss: 0.7095\n",
      "  Learning Rate: 0.000099\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 2000/2000 [27:21<00:00,  1.22it/s, loss=2.6646, avg_loss=0.7596, lr=0.000099, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100 Summary:\n",
      "  Average Loss: 0.7596\n",
      "  Learning Rate: 0.000098\n",
      "------------------------------------------------------------\n",
      "✓ Checkpoint saved: sassl_checkpoint_epoch_8.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 2000/2000 [27:22<00:00,  1.22it/s, loss=0.0120, avg_loss=0.7073, lr=0.000098, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100 Summary:\n",
      "  Average Loss: 0.7073\n",
      "  Learning Rate: 0.000098\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 2000/2000 [27:28<00:00,  1.21it/s, loss=0.7083, avg_loss=0.6272, lr=0.000098, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100 Summary:\n",
      "  Average Loss: 0.6272\n",
      "  Learning Rate: 0.000098\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 2000/2000 [27:16<00:00,  1.22it/s, loss=0.1377, avg_loss=0.5851, lr=0.000098, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100 Summary:\n",
      "  Average Loss: 0.5851\n",
      "  Learning Rate: 0.000097\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 2000/2000 [27:30<00:00,  1.21it/s, loss=0.6318, avg_loss=0.5898, lr=0.000097, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100 Summary:\n",
      "  Average Loss: 0.5898\n",
      "  Learning Rate: 0.000097\n",
      "------------------------------------------------------------\n",
      "✓ Checkpoint saved: sassl_checkpoint_epoch_12.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 2000/2000 [27:30<00:00,  1.21it/s, loss=0.7144, avg_loss=0.6637, lr=0.000097, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100 Summary:\n",
      "  Average Loss: 0.6637\n",
      "  Learning Rate: 0.000096\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 2000/2000 [27:29<00:00,  1.21it/s, loss=0.7828, avg_loss=0.6440, lr=0.000096, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100 Summary:\n",
      "  Average Loss: 0.6440\n",
      "  Learning Rate: 0.000095\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 2000/2000 [27:27<00:00,  1.21it/s, loss=0.6932, avg_loss=0.6631, lr=0.000095, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100 Summary:\n",
      "  Average Loss: 0.6631\n",
      "  Learning Rate: 0.000095\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 2000/2000 [27:26<00:00,  1.21it/s, loss=0.5514, avg_loss=0.6888, lr=0.000095, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/100 Summary:\n",
      "  Average Loss: 0.6888\n",
      "  Learning Rate: 0.000094\n",
      "------------------------------------------------------------\n",
      "✓ Checkpoint saved: sassl_checkpoint_epoch_16.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 2000/2000 [27:23<00:00,  1.22it/s, loss=0.2182, avg_loss=0.7680, lr=0.000094, momentum=0.9960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/100 Summary:\n",
      "  Average Loss: 0.7680\n",
      "  Learning Rate: 0.000093\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 2000/2000 [1:26:47<00:00,  2.60s/it, loss=0.6977, avg_loss=0.7647, lr=0.000093, momentum=0.9960]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/100 Summary:\n",
      "  Average Loss: 0.7647\n",
      "  Learning Rate: 0.000092\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 2000/2000 [35:54<00:00,  1.08s/it, loss=0.3059, avg_loss=0.6802, lr=0.000092, momentum=0.9960] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/100 Summary:\n",
      "  Average Loss: 0.6802\n",
      "  Learning Rate: 0.000091\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 2000/2000 [46:11<00:00,  1.39s/it, loss=0.9080, avg_loss=0.6465, lr=0.000091, momentum=0.9960] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100 Summary:\n",
      "  Average Loss: 0.6465\n",
      "  Learning Rate: 0.000091\n",
      "------------------------------------------------------------\n",
      "✓ Checkpoint saved: sassl_checkpoint_epoch_20.pth\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100:   0%|          | 0/2000 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     69\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(trainLoader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Skip if batch size < 2 (need negatives)\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:494\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:427\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1170\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1163\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\torch\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from swinSASSL import SwinSASSL\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 1e-4\n",
    "momentum_teacher = 0.996\n",
    "warmup_epochs = 5\n",
    "weight_decay = 0.04\n",
    "\n",
    "# Initialize model and loss\n",
    "model = SwinSASSL(\n",
    "    random_crop_size=(64, 64),\n",
    "    drop_probability=0.4,\n",
    "    swin_in_channels=9\n",
    ")\n",
    "\n",
    "criterion = InfoNCELoss(temperature=0.07)\n",
    "\n",
    "# Optimizer (only student parameters)\n",
    "optimizer = optim.AdamW(\n",
    "    model.student.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Non-trainable parameters: {total - trainable:,}\")\n",
    "print(f\"Percentage trainable: {100 * trainable / total:.2f}%\")\n",
    "\n",
    "# More detailed breakdown\n",
    "print(\"\\n--- Student Model ---\")\n",
    "student_total, student_trainable = count_parameters(model.student)\n",
    "print(f\"Student total: {student_total:,}\")\n",
    "print(f\"Student trainable: {student_trainable:,}\")\n",
    "\n",
    "print(\"\\n--- Teacher Model ---\")\n",
    "teacher_total, teacher_trainable = count_parameters(model.teacher)\n",
    "print(f\"Teacher total: {teacher_total:,}\")\n",
    "print(f\"Teacher trainable: {teacher_trainable:,}\")\n",
    "\n",
    "# Training loop\n",
    "model.student.train()\n",
    "model.teacher.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(trainLoader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        images = batch['c9'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Skip if batch size < 2 (need negatives)\n",
    "        if images.shape[0] < 2:\n",
    "            continue\n",
    "        \n",
    "        # Forward pass (images moved to device inside model)\n",
    "        teacher_outputs, student_outputs = model(images)\n",
    "        # print(teacher_outputs.shape)\n",
    "        # print(student_outputs[0].shape)\n",
    "        \n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = criterion(teacher_outputs, student_outputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.student.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update student\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update teacher with EMA\n",
    "        # Momentum warmup: gradually increase from 0 to target momentum\n",
    "        if epoch < warmup_epochs:\n",
    "            m = momentum_teacher * (epoch / warmup_epochs)\n",
    "        else:\n",
    "            m = momentum_teacher\n",
    "        \n",
    "        model.update_teacher(momentum=m)\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{epoch_loss/num_batches:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}',\n",
    "            'momentum': f'{m:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate average epoch loss\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 4 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'student_state_dict': model.student.state_dict(),\n",
    "            'teacher_state_dict': model.teacher.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f'sassl_checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"✓ Checkpoint saved: sassl_checkpoint_epoch_{epoch+1}.pth\\n\")\n",
    "\n",
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'student_state_dict': model.student.state_dict(),\n",
    "    'teacher_state_dict': model.teacher.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'final_loss': avg_loss,\n",
    "}\n",
    "torch.save(final_checkpoint, 'sassl_final_model.pth')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final model saved: sassl_final_model.pth\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
