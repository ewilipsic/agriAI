{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f8330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 2526/4640 [04:25<03:42,  9.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m data_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/s2a.tar/s2a\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# In train.py, use target_size that's divisible by 32:\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSentinel2InpaintingDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or (256, 256), (384, 384), etc.\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ayush\\OneDrive\\Desktop\\Agriculture\\dataset.py:48\u001b[0m, in \u001b[0;36mSentinel2InpaintingDataset.__init__\u001b[1;34m(self, root_dir, mask_type, augment, target_size)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(timestamp_folder):\n\u001b[0;32m     45\u001b[0m                 \u001b[38;5;66;03m# Check if all bands exist\u001b[39;00m\n\u001b[0;32m     46\u001b[0m                 band_paths \u001b[38;5;241m=\u001b[39m {band: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(timestamp_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mband\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     47\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m band \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbands}\n\u001b[1;32m---> 48\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m band_paths\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m     49\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mappend(band_paths)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples with all 12 bands\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ayush\\OneDrive\\Desktop\\Agriculture\\dataset.py:48\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(timestamp_folder):\n\u001b[0;32m     45\u001b[0m                 \u001b[38;5;66;03m# Check if all bands exist\u001b[39;00m\n\u001b[0;32m     46\u001b[0m                 band_paths \u001b[38;5;241m=\u001b[39m {band: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(timestamp_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mband\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     47\u001b[0m                              \u001b[38;5;28;01mfor\u001b[39;00m band \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbands}\n\u001b[1;32m---> 48\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m band_paths\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m     49\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mappend(band_paths)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples with all 12 bands\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from dataset import Sentinel2InpaintingDataset\n",
    "from model import ConvAutoEncoder, UNetAutoEncoder\n",
    "\n",
    "data_root='D:/s2a.tar/s2a'\n",
    "\n",
    "# In train.py, use target_size that's divisible by 32:\n",
    "dataset = Sentinel2InpaintingDataset(\n",
    "    root_dir=data_root,\n",
    "    mask_type='random',\n",
    "    augment=True,\n",
    "    target_size=(256, 256)  # or (256, 256), (384, 384), etc.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ddd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Creating simple model...\n",
      "Model parameters: 5,476,236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   2%|▏         | 100/5366 [01:18<1:08:34,  1.28it/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.001507, Val Loss = 0.000819\n",
      "Saved best model with val_loss = 0.000819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   2%|▏         | 100/5366 [01:17<1:08:16,  1.29it/s, loss=0.0411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.000772, Val Loss = 0.000550\n",
      "Saved best model with val_loss = 0.000550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 100/5366 [01:15<1:06:33,  1.32it/s, loss=0.0229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.000531, Val Loss = 0.000510\n",
      "Saved best model with val_loss = 0.000510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   2%|▏         | 100/5366 [01:16<1:06:50,  1.31it/s, loss=0.0248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.000441, Val Loss = 0.000466\n",
      "Saved best model with val_loss = 0.000466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   2%|▏         | 100/5366 [01:18<1:08:53,  1.27it/s, loss=0.0467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.000494, Val Loss = 0.000358\n",
      "Saved best model with val_loss = 0.000358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   2%|▏         | 100/5366 [01:11<1:02:46,  1.40it/s, loss=0.00936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.000348, Val Loss = 0.000325\n",
      "Saved best model with val_loss = 0.000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   2%|▏         | 100/5366 [01:07<59:32,  1.47it/s, loss=0.00972] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.000338, Val Loss = 0.000281\n",
      "Saved best model with val_loss = 0.000281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   2%|▏         | 100/5366 [01:10<1:02:12,  1.41it/s, loss=0.0304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.000261, Val Loss = 0.000238\n",
      "Saved best model with val_loss = 0.000238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   2%|▏         | 100/5366 [01:08<59:48,  1.47it/s, loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.000256, Val Loss = 0.000229\n",
      "Saved best model with val_loss = 0.000229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   2%|▏         | 100/5366 [01:13<1:04:45,  1.36it/s, loss=0.0245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.000206, Val Loss = 0.000197\n",
      "Saved best model with val_loss = 0.000197\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(\n",
    "    data_root='D:/s2a',\n",
    "    model_type='unet',  # 'simple' or 'unet'\n",
    "    batch_size=4,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    latent_dim=256,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    save_dir='checkpoints'\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    \n",
    "    # Train/val split (90/10)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory= True,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(f\"Creating {model_type} model...\")\n",
    "    if model_type == 'simple':\n",
    "        model = ConvAutoEncoder(latent_dim=latent_dim)\n",
    "    else:\n",
    "        model = UNetAutoEncoder(latent_dim=latent_dim)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        batch_count = 0\n",
    "        for batch in pbar:\n",
    "            batch_count += 1\n",
    "            if(batch_count > 100): break\n",
    "            original = batch['original'].to(device)\n",
    "            masked = batch['masked'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon, latent = model(masked)\n",
    "            print(masked.shape)\n",
    "            \n",
    "            # Calculate loss (can weight the masked region more)\n",
    "            loss = criterion(recon, original)\n",
    "            \n",
    "            # Optional: Focus more on masked regions\n",
    "            # masked_region_loss = criterion(recon * (1 - mask), original * (1 - mask))\n",
    "            # loss = 0.7 * loss + 0.3 * masked_region_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_count = 0\n",
    "            for batch in val_loader:\n",
    "                batch_count += 1\n",
    "                if(batch_count > 10): break\n",
    "                original = batch['original'].to(device)\n",
    "                masked = batch['masked'].to(device)\n",
    "                \n",
    "                recon, latent = model(masked)\n",
    "                loss = criterion(recon, original)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # # Save best model\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     torch.save({\n",
    "        #         'epoch': epoch,\n",
    "        #         'model_state_dict': model.state_dict(),\n",
    "        #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #         'train_loss': train_loss,\n",
    "        #         'val_loss': val_loss,\n",
    "        #     }, os.path.join(save_dir, 'best_model.pth'))\n",
    "        #     print(f'Saved best model with val_loss = {val_loss:.6f}')\n",
    "        \n",
    "        # # Save checkpoint every 10 epochs\n",
    "        # if (epoch + 1) % 1 == 0:\n",
    "        #     torch.save({\n",
    "        #         'epoch': epoch,\n",
    "        #         'model_state_dict': model.state_dict(),\n",
    "        #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #         'train_loss': train_loss,\n",
    "        #         'val_loss': val_loss,\n",
    "        #     }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training Progress')\n",
    "    plt.savefig(os.path.join(save_dir, 'training_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = train_autoencoder(\n",
    "        data_root='D:/s2a.tar/s2a',\n",
    "        model_type='simple',  # Use U-Net for better results\n",
    "        batch_size=4,       # Adjust based on GPU memory\n",
    "        num_epochs=10,\n",
    "        learning_rate=1e-4,\n",
    "        latent_dim=256\n",
    "    )\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
